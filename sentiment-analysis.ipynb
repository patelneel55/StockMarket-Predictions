{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "version": "2.7.17-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython2",
  "version": 2,
  "kernelspec": {
   "name": "python36964bit1cea69b2233644e59d0aa89b42c110e3",
   "display_name": "Python 3.6.9 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Company Sentiment Analysis"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "# nltk.download('vader_lexicon')\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update Vader Lexicon to accomodate financial vocabulary"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "# Stock Market Lexicon (https://github.com/nunomroliveira/stock_market_lexicon/blob/master/stock_lex.csv)\n",
    "stock_lex = pd.read_csv('lex_data/stock_lex.csv')\n",
    "stock_lex['sentiment'] = (stock_lex['Aff_Score'] + stock_lex['Neg_Score'])/2\n",
    "stock_lex = dict(zip(stock_lex.Item, stock_lex.sentiment))\n",
    "\n",
    "stock_lex = {k:v for k,v in stock_lex.items() if len(k.split(' '))==1}\n",
    "\n",
    "stock_lex_scaled = {}\n",
    "for k, v in stock_lex.items():\n",
    "    if v > 0:\n",
    "        stock_lex_scaled[k] = v / max(stock_lex.values()) * 4\n",
    "    else:\n",
    "        stock_lex_scaled[k] = v / min(stock_lex.values()) * -4\n",
    "\n",
    "# # Loughran McDonald Lexicon ()\n",
    "positive = []\n",
    "with open('lex_data/positive.csv', 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        positive.append(row[0].strip())\n",
    "    \n",
    "negative = []\n",
    "with open('lex_data/negative.csv', 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        entry = row[0].strip().split(\" \")\n",
    "        if len(entry) > 1:\n",
    "            negative.extend(entry)\n",
    "        else:\n",
    "            negative.append(entry[0])\n",
    "\n",
    "final_lex = {}\n",
    "final_lex.update({word:2.0 for word in positive})\n",
    "final_lex.update({word:-2.0 for word in negative})\n",
    "final_lex.update(stock_lex_scaled)\n",
    "final_lex.update(sia.lexicon)\n",
    "sia.lexicon = final_lex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Vader to retrieve sentiment score"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables that determine data collection criteria\n",
    "keywords = 'Goldman Sachs'\n",
    "from_date = '2010-01-01' \n",
    "to_date = '2010-02-01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import pprint\n",
    "\n",
    "date_sentiments = {}\n",
    "\n",
    "for i in range(1,3):\n",
    "    page = urlopen('https://www.businesstimes.com.sg/search/facebook?page='+str(i)).read()\n",
    "    soup = BeautifulSoup(page, features=\"html.parser\")\n",
    "    posts = soup.findAll(\"div\", {\"class\": \"media-body\"})\n",
    "    for post in posts:\n",
    "        time.sleep(1)\n",
    "        url = post.a['href']\n",
    "        date = post.time.text\n",
    "        print(date, url)\n",
    "        try:\n",
    "            link_page = urlopen(url).read()\n",
    "        except:\n",
    "            url = url[:-2]\n",
    "            link_page = urlopen(url).read()\n",
    "        link_soup = BeautifulSoup(link_page)\n",
    "        sentences = link_soup.findAll(\"p\")\n",
    "        passage = \"\"\n",
    "        for sentence in sentences:\n",
    "            passage += sentence.text\n",
    "        sentiment = sia.polarity_scores(passage)['compound']\n",
    "        date_sentiments.setdefault(date, []).append(sentiment)\n",
    "    print(date_sentiments)\n",
    "\n",
    "date_sentiment = {}\n",
    "\n",
    "for k,v in date_sentiments.items():\n",
    "    date_sentiment[datetime.strptime(k, '%d %b %Y').date() + timedelta(days=1)] = round(sum(v)/float(len(v)),3)\n",
    "\n",
    "earliest_date = min(date_sentiment.keys())\n",
    "\n",
    "print(date_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "https://www.google.com/search?hl=en&gl=us&tbm=nws&authuser=0&q=Goldman Sachs&tbs=cdr:1,cd_min:01/01/2010,cd_max:01/01/2010,sbd:1\n2010-01-01 0.743\nhttps://www.google.com/search?hl=en&gl=us&tbm=nws&authuser=0&q=Goldman Sachs&tbs=cdr:1,cd_min:01/02/2010,cd_max:01/02/2010,sbd:1\n2010-01-02 0\nhttps://www.google.com/search?hl=en&gl=us&tbm=nws&authuser=0&q=Goldman Sachs&tbs=cdr:1,cd_min:01/03/2010,cd_max:01/03/2010,sbd:1\n2010-01-03 0.999\nhttps://www.google.com/search?hl=en&gl=us&tbm=nws&authuser=0&q=Goldman Sachs&tbs=cdr:1,cd_min:01/04/2010,cd_max:01/04/2010,sbd:1\n2010-01-04 0.992\nhttps://www.google.com/search?hl=en&gl=us&tbm=nws&authuser=0&q=Goldman Sachs&tbs=cdr:1,cd_min:01/05/2010,cd_max:01/05/2010,sbd:1\n2010-01-05 0.733\nhttps://www.google.com/search?hl=en&gl=us&tbm=nws&authuser=0&q=Goldman Sachs&tbs=cdr:1,cd_min:01/06/2010,cd_max:01/06/2010,sbd:1\n2010-01-06 0.983\nhttps://www.google.com/search?hl=en&gl=us&tbm=nws&authuser=0&q=Goldman Sachs&tbs=cdr:1,cd_min:01/07/2010,cd_max:01/07/2010,sbd:1\n2010-01-07 0.78\nhttps://www.google.com/search?hl=en&gl=us&tbm=nws&authuser=0&q=Goldman Sachs&tbs=cdr:1,cd_min:01/08/2010,cd_max:01/08/2010,sbd:1\n2010-01-08 0.992\nhttps://www.google.com/search?hl=en&gl=us&tbm=nws&authuser=0&q=Goldman Sachs&tbs=cdr:1,cd_min:01/09/2010,cd_max:01/09/2010,sbd:1\n2010-01-09 0.996\nhttps://www.google.com/search?hl=en&gl=us&tbm=nws&authuser=0&q=Goldman Sachs&tbs=cdr:1,cd_min:01/10/2010,cd_max:01/10/2010,sbd:1\n2010-01-10 1.0\nhttps://www.google.com/search?hl=en&gl=us&tbm=nws&authuser=0&q=Goldman Sachs&tbs=cdr:1,cd_min:01/11/2010,cd_max:01/11/2010,sbd:1\n2010-01-11 0.924\nhttps://www.google.com/search?hl=en&gl=us&tbm=nws&authuser=0&q=Goldman Sachs&tbs=cdr:1,cd_min:01/12/2010,cd_max:01/12/2010,sbd:1\n2010-01-12 0.986\nhttps://www.google.com/search?hl=en&gl=us&tbm=nws&authuser=0&q=Goldman Sachs&tbs=cdr:1,cd_min:01/13/2010,cd_max:01/13/2010,sbd:1\n2010-01-13 0.797\nhttps://www.google.com/search?hl=en&gl=us&tbm=nws&authuser=0&q=Goldman Sachs&tbs=cdr:1,cd_min:01/14/2010,cd_max:01/14/2010,sbd:1\n2010-01-14 0.913\nhttps://www.google.com/search?hl=en&gl=us&tbm=nws&authuser=0&q=Goldman Sachs&tbs=cdr:1,cd_min:01/15/2010,cd_max:01/15/2010,sbd:1\n2010-01-15 0.621\nhttps://www.google.com/search?hl=en&gl=us&tbm=nws&authuser=0&q=Goldman Sachs&tbs=cdr:1,cd_min:01/16/2010,cd_max:01/16/2010,sbd:1\n2010-01-16 0\nhttps://www.google.com/search?hl=en&gl=us&tbm=nws&authuser=0&q=Goldman Sachs&tbs=cdr:1,cd_min:01/17/2010,cd_max:01/17/2010,sbd:1\n2010-01-17 0.938\nhttps://www.google.com/search?hl=en&gl=us&tbm=nws&authuser=0&q=Goldman Sachs&tbs=cdr:1,cd_min:01/18/2010,cd_max:01/18/2010,sbd:1\n2010-01-18 0.846\nhttps://www.google.com/search?hl=en&gl=us&tbm=nws&authuser=0&q=Goldman Sachs&tbs=cdr:1,cd_min:01/19/2010,cd_max:01/19/2010,sbd:1\n2010-01-19 0.797\nhttps://www.google.com/search?hl=en&gl=us&tbm=nws&authuser=0&q=Goldman Sachs&tbs=cdr:1,cd_min:01/20/2010,cd_max:01/20/2010,sbd:1\n2010-01-20 0.677\nhttps://www.google.com/search?hl=en&gl=us&tbm=nws&authuser=0&q=Goldman Sachs&tbs=cdr:1,cd_min:01/21/2010,cd_max:01/21/2010,sbd:1\n2010-01-21 0.877\nhttps://www.google.com/search?hl=en&gl=us&tbm=nws&authuser=0&q=Goldman Sachs&tbs=cdr:1,cd_min:01/22/2010,cd_max:01/22/2010,sbd:1\n2010-01-22 0.458\nhttps://www.google.com/search?hl=en&gl=us&tbm=nws&authuser=0&q=Goldman Sachs&tbs=cdr:1,cd_min:01/23/2010,cd_max:01/23/2010,sbd:1\n2010-01-23 0.476\nhttps://www.google.com/search?hl=en&gl=us&tbm=nws&authuser=0&q=Goldman Sachs&tbs=cdr:1,cd_min:01/24/2010,cd_max:01/24/2010,sbd:1\n2010-01-24 0.706\nhttps://www.google.com/search?hl=en&gl=us&tbm=nws&authuser=0&q=Goldman Sachs&tbs=cdr:1,cd_min:01/25/2010,cd_max:01/25/2010,sbd:1\n2010-01-25 0.963\nhttps://www.google.com/search?hl=en&gl=us&tbm=nws&authuser=0&q=Goldman Sachs&tbs=cdr:1,cd_min:01/26/2010,cd_max:01/26/2010,sbd:1\n2010-01-26 0.968\nhttps://www.google.com/search?hl=en&gl=us&tbm=nws&authuser=0&q=Goldman Sachs&tbs=cdr:1,cd_min:01/27/2010,cd_max:01/27/2010,sbd:1\n2010-01-27 0.723\nhttps://www.google.com/search?hl=en&gl=us&tbm=nws&authuser=0&q=Goldman Sachs&tbs=cdr:1,cd_min:01/28/2010,cd_max:01/28/2010,sbd:1\n2010-01-28 0.886\nhttps://www.google.com/search?hl=en&gl=us&tbm=nws&authuser=0&q=Goldman Sachs&tbs=cdr:1,cd_min:01/29/2010,cd_max:01/29/2010,sbd:1\n2010-01-29 0.864\nhttps://www.google.com/search?hl=en&gl=us&tbm=nws&authuser=0&q=Goldman Sachs&tbs=cdr:1,cd_min:01/30/2010,cd_max:01/30/2010,sbd:1\n2010-01-30 -0.831\nhttps://www.google.com/search?hl=en&gl=us&tbm=nws&authuser=0&q=Goldman Sachs&tbs=cdr:1,cd_min:01/31/2010,cd_max:01/31/2010,sbd:1\n2010-01-31 1.0\nhttps://www.google.com/search?hl=en&gl=us&tbm=nws&authuser=0&q=Goldman Sachs&tbs=cdr:1,cd_min:02/01/2010,cd_max:02/01/2010,sbd:1\n2010-02-01 0.778\n\n{'2009-12-31': 0.743, '2010-01-01': 0, '2010-01-02': 0.999, '2010-01-03': 0.992, '2010-01-04': 0.733, '2010-01-05': 0.983, '2010-01-06': 0.78, '2010-01-07': 0.992, '2010-01-08': 0.996, '2010-01-09': 1.0, '2010-01-10': 0.924, '2010-01-11': 0.986, '2010-01-12': 0.797, '2010-01-13': 0.913, '2010-01-14': 0.621, '2010-01-15': 0, '2010-01-16': 0.938, '2010-01-17': 0.846, '2010-01-18': 0.797, '2010-01-19': 0.677, '2010-01-20': 0.877, '2010-01-21': 0.458, '2010-01-22': 0.476, '2010-01-23': 0.706, '2010-01-24': 0.963, '2010-01-25': 0.968, '2010-01-26': 0.723, '2010-01-27': 0.886, '2010-01-28': 0.864, '2010-01-29': -0.831, '2010-01-30': 1.0, '2010-01-31': 0.778}\n"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timedelta\n",
    "from time import sleep\n",
    "import csv\n",
    "\n",
    "# Scrape URL's from Google News results for the provided keywords and date\n",
    "def get_news(keywords, date):\n",
    "    url = \"https://www.google.com/search?hl=en&gl=us&tbm=nws&authuser=0&q=\" + keywords + \"&tbs=cdr:1,cd_min:\" + date + \",cd_max:\" + date + \",sbd:1\"\n",
    "    hdrs = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36',\n",
    "       'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "       'Accept-Charset': 'ISO-8859-1,utf-8;q=0.7,*;q=0.3',\n",
    "       'Accept-Encoding': 'none',\n",
    "       'Accept-Language': 'en-US,en;q=0.8',\n",
    "       'Connection': 'keep-alive'}\n",
    "\n",
    "    print(url)\n",
    "\n",
    "    response = None\n",
    "    try:\n",
    "        response = requests.get(url, headers=hdrs)\n",
    "    except requests.ConnectionError as e:\n",
    "        with open('log.txt', 'a') as log_file:\n",
    "            log_file.write(datetime.today().isoformat())\n",
    "            log_file.write(url)\n",
    "        return []\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, features=\"html.parser\")\n",
    "    result_div = soup.find_all('div', attrs = {'class': 'dbsr'})\n",
    "\n",
    "    links = []\n",
    "    for r in result_div:\n",
    "        # Checks if each element is present, else, raise exception\n",
    "        try:\n",
    "            link = r.find('a', href = True)\n",
    "            # title = r.find('div', attrs={'class':'vvjwJb'}).get_text()\n",
    "            # description = r.find('div', attrs={'class':'s3v9rd'}).get_text()\n",
    "            \n",
    "            # Check to make sure everything is present before appending\n",
    "            if link != '':\n",
    "                links.append(link['href'])\n",
    "\n",
    "        # Next loop if one element is not present\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    return links\n",
    "\n",
    "# Get number of days for the date range\n",
    "end_date_obj = datetime.strptime(to_date, '%Y-%m-%d')\n",
    "start_date_obj = datetime.strptime(from_date, '%Y-%m-%d')\n",
    "num_days = (end_date_obj - start_date_obj).days + 1\n",
    "\n",
    "date_sentiment = {}\n",
    "\n",
    "# Get news articles for every date and calculate sentiment score\n",
    "for date in (start_date_obj + timedelta(days=n) for n in range(num_days)):\n",
    "    sleep(0.05)\n",
    "    # print(date.strftime(\"%m/%d/%Y\"))\n",
    "    news_urls = get_news(keywords=keywords, date=str(date.strftime(\"%m/%d/%Y\")))\n",
    "    \n",
    "    sentiment_avg = 0\n",
    "    for url in news_urls:\n",
    "\n",
    "        # Scrape article content\n",
    "        link_page = None\n",
    "        try:\n",
    "            link_page = requests.get(url)\n",
    "        except requests.ConnectionError as e:\n",
    "            with open('log.txt', 'a') as log_file:\n",
    "                log_file.write(datetime.today().isoformat())\n",
    "                log_file.write(url)\n",
    "            continue\n",
    "\n",
    "        link_soup = BeautifulSoup(link_page.text)\n",
    "        sentences = link_soup.findAll(\"p\")\n",
    "        passage = \"\"\n",
    "        for sentence in sentences:\n",
    "            passage += sentence.text\n",
    "\n",
    "        sentiment_avg += sia.polarity_scores(passage)['compound']\n",
    "    \n",
    "    sentiment_avg = 0 if len(news_urls) == 0 else round(sentiment_avg / len(news_urls), 3)\n",
    "\n",
    "\n",
    "    print(date.date(), sentiment_avg)\n",
    "    with open('urls.txt', 'a') as url_file:\n",
    "        url_file.write(str(date.date()) + '\\t(' + str(sentiment_avg) + ')\\n\\n')\n",
    "        url_file.writelines(\"%s\\n\" % u for u in news_urls)\n",
    "        url_file.write('\\n\\n')\n",
    "    \n",
    "    date_sentiment[str(date.date() - timedelta(days=1))] = sentiment_avg\n",
    "\n",
    "print()\n",
    "print(date_sentiment)\n",
    "\n",
    "with open('sentiment.csv', 'a') as sent_file:\n",
    "    writer = csv.writer(sent_file)\n",
    "    for key,value in date_sentiment.items():\n",
    "        writer.writerow([key,value])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}