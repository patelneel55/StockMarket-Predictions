{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "version": "2.7.17-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython2",
  "version": 2,
  "kernelspec": {
   "name": "python36964bit1cea69b2233644e59d0aa89b42c110e3",
   "display_name": "Python 3.6.9 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Company Sentiment Analysis"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "# nltk.download('vader_lexicon')\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update Vader Lexicon to accomodate financial vocabulary"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "# Stock Market Lexicon (https://github.com/nunomroliveira/stock_market_lexicon/blob/master/stock_lex.csv)\n",
    "stock_lex = pd.read_csv('lex_data/stock_lex.csv')\n",
    "stock_lex['sentiment'] = (stock_lex['Aff_Score'] + stock_lex['Neg_Score'])/2\n",
    "stock_lex = dict(zip(stock_lex.Item, stock_lex.sentiment))\n",
    "\n",
    "stock_lex = {k:v for k,v in stock_lex.items() if len(k.split(' '))==1}\n",
    "\n",
    "stock_lex_scaled = {}\n",
    "for k, v in stock_lex.items():\n",
    "    if v > 0:\n",
    "        stock_lex_scaled[k] = v / max(stock_lex.values()) * 4\n",
    "    else:\n",
    "        stock_lex_scaled[k] = v / min(stock_lex.values()) * -4\n",
    "\n",
    "# # Loughran McDonald Lexicon ()\n",
    "positive = []\n",
    "with open('lex_data/positive.csv', 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        positive.append(row[0].strip())\n",
    "    \n",
    "negative = []\n",
    "with open('lex_data/negative.csv', 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        entry = row[0].strip().split(\" \")\n",
    "        if len(entry) > 1:\n",
    "            negative.extend(entry)\n",
    "        else:\n",
    "            negative.append(entry[0])\n",
    "\n",
    "final_lex = {}\n",
    "final_lex.update({word:2.0 for word in positive})\n",
    "final_lex.update({word:-2.0 for word in negative})\n",
    "final_lex.update(stock_lex_scaled)\n",
    "final_lex.update(sia.lexicon)\n",
    "sia.lexicon = final_lex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Vader to retrieve sentiment score"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables that determine data collection criteria\n",
    "keywords = 'Goldman Sachs'\n",
    "from_date = '2010-02-02' \n",
    "to_date = '2010-03-03'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import pprint\n",
    "\n",
    "date_sentiments = {}\n",
    "\n",
    "for i in range(1,3):\n",
    "    page = urlopen('https://www.businesstimes.com.sg/search/facebook?page='+str(i)).read()\n",
    "    soup = BeautifulSoup(page, features=\"html.parser\")\n",
    "    posts = soup.findAll(\"div\", {\"class\": \"media-body\"})\n",
    "    for post in posts:\n",
    "        time.sleep(1)\n",
    "        url = post.a['href']\n",
    "        date = post.time.text\n",
    "        print(date, url)\n",
    "        try:\n",
    "            link_page = urlopen(url).read()\n",
    "        except:\n",
    "            url = url[:-2]\n",
    "            link_page = urlopen(url).read()\n",
    "        link_soup = BeautifulSoup(link_page)\n",
    "        sentences = link_soup.findAll(\"p\")\n",
    "        passage = \"\"\n",
    "        for sentence in sentences:\n",
    "            passage += sentence.text\n",
    "        sentiment = sia.polarity_scores(passage)['compound']\n",
    "        date_sentiments.setdefault(date, []).append(sentiment)\n",
    "    print(date_sentiments)\n",
    "\n",
    "date_sentiment = {}\n",
    "\n",
    "for k,v in date_sentiments.items():\n",
    "    date_sentiment[datetime.strptime(k, '%d %b %Y').date() + timedelta(days=1)] = round(sum(v)/float(len(v)),3)\n",
    "\n",
    "earliest_date = min(date_sentiment.keys())\n",
    "\n",
    "print(date_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timedelta\n",
    "from textblob import TextBlob\n",
    "from time import sleep\n",
    "import csv\n",
    "\n",
    "# Scrape URL's from Google News results for the provided keywords and date\n",
    "def get_news(keywords, date):\n",
    "    url = \"https://www.google.com/search?hl=en&gl=us&tbm=nws&authuser=0&q=\" + keywords + \"&tbs=cdr:1,cd_min:\" + date + \",cd_max:\" + date + \",sbd:1\"\n",
    "    hdrs = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36',\n",
    "       'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "       'Accept-Charset': 'ISO-8859-1,utf-8;q=0.7,*;q=0.3',\n",
    "       'Accept-Encoding': 'none',\n",
    "       'Accept-Language': 'en-US,en;q=0.8',\n",
    "       'Connection': 'keep-alive'}\n",
    "\n",
    "    print(url)\n",
    "\n",
    "    response = None\n",
    "    try:\n",
    "        response = requests.get(url, headers=hdrs)\n",
    "    except requests.ConnectionError as e:\n",
    "        with open('log.txt', 'a') as log_file:\n",
    "            log_file.write(datetime.today().isoformat())\n",
    "            log_file.write(url)\n",
    "        return []\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, features=\"html.parser\")\n",
    "    result_div = soup.find_all('div', attrs = {'class': 'dbsr'})\n",
    "\n",
    "    links = []\n",
    "    for r in result_div:\n",
    "        # Checks if each element is present, else, raise exception\n",
    "        try:\n",
    "            link = r.find('a', href = True)\n",
    "            # title = r.find('div', attrs={'class':'vvjwJb'}).get_text()\n",
    "            # description = r.find('div', attrs={'class':'s3v9rd'}).get_text()\n",
    "            \n",
    "            # Check to make sure everything is present before appending\n",
    "            if link != '':\n",
    "                links.append(link['href'])\n",
    "\n",
    "        # Next loop if one element is not present\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    return links\n",
    "\n",
    "# Get number of days for the date range\n",
    "end_date_obj = datetime.strptime(to_date, '%Y-%m-%d')\n",
    "start_date_obj = datetime.strptime(from_date, '%Y-%m-%d')\n",
    "num_days = (end_date_obj - start_date_obj).days + 1\n",
    "\n",
    "date_sentiment = {}\n",
    "\n",
    "print(num_days)\n",
    "\n",
    "# Get news articles for every date and calculate sentiment score\n",
    "for date in (start_date_obj + timedelta(days=n) for n in range(num_days)):\n",
    "    sleep(0.05)\n",
    "    # print(date.strftime(\"%m/%d/%Y\"))\n",
    "    news_urls = get_news(keywords=keywords, date=str(date.strftime(\"%m/%d/%Y\")))\n",
    "    \n",
    "    sentiment_avg = 0\n",
    "    sentiment_avg2 = 0\n",
    "    for url in news_urls:\n",
    "\n",
    "        # Scrape article content\n",
    "        link_page = None\n",
    "        try:\n",
    "            link_page = requests.get(url)\n",
    "        except requests.ConnectionError as e:\n",
    "            with open('log.txt', 'a') as log_file:\n",
    "                log_file.write(datetime.today().isoformat())\n",
    "                log_file.write(url)\n",
    "            continue\n",
    "\n",
    "        link_soup = BeautifulSoup(link_page.text)\n",
    "        sentences = link_soup.findAll(\"p\")\n",
    "        passage = \"\"\n",
    "        for sentence in sentences:\n",
    "            passage += sentence.text\n",
    "\n",
    "        sentiment_avg += sia.polarity_scores(passage)['compound']\n",
    "        sentiment_avg2 += TextBlob(passage).sentiment.polarity\n",
    "    \n",
    "    sentiment_avg = 0 if len(news_urls) == 0 else round(sentiment_avg / len(news_urls), 3)\n",
    "    sentiment_avg2 = 0 if len(news_urls) == 0 else round(sentiment_avg2 / len(news_urls), 3)\n",
    "\n",
    "\n",
    "    print(date.date(), sentiment_avg, sentiment_avg2)\n",
    "    with open('urls.txt', 'a') as url_file:\n",
    "        url_file.write(str(date.date()) + '\\t(' + str(sentiment_avg) + ')\\n\\n')\n",
    "        url_file.writelines(\"%s\\n\" % u for u in news_urls)\n",
    "        url_file.write('\\n\\n')\n",
    "    \n",
    "    date_sentiment[str(date.date() - timedelta(days=1))] = (sentiment_avg, sentiment_avg2)\n",
    "\n",
    "print()\n",
    "print(date_sentiment)\n",
    "\n",
    "with open('sentiment.csv', 'a') as sent_file:\n",
    "    writer = csv.writer(sent_file)\n",
    "    for key,value in date_sentiment.items():\n",
    "        writer.writerow([key,value[0], value[1]])\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gather sentiment data from Twitter API"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "True"
     },
     "metadata": {},
     "execution_count": 98
    }
   ],
   "source": [
    "import tweepy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import twitter\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter Data Collection"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize Twitter API access\n",
    "\n",
    "\"\"\" Initialize Twitter Authentication & set up API client \"\"\"\n",
    "try:\n",
    "    consumer_key = os.environ['TWITTER_CONSUMER_KEY']\n",
    "    consumer_secret = os.environ['TWITTER_CONSUMER_SECRET']\n",
    "    access_token = os.environ['TWITTER_ACCESS_TOKEN']\n",
    "    access_secret = os.environ['TWITTER_ACCESS_SECRET']\n",
    "except KeyError:\n",
    "    sys.stderr.write(\"TWITTER_* environment variables not set\\n\")\n",
    "    sys.exit(1)\n",
    "# auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "# auth.set_access_token(access_token, access_secret)\n",
    "\n",
    "## Setup twitter API Client ##\n",
    "# twitter_api = tweepy.API(auth, wait_on_rate_limit=True)\n",
    "twitter_api = twitter.Api(consumer_key=consumer_key, consumer_secret=consumer_secret, access_token_key=access_token, access_token_secret=access_secret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('temp.jsonl', 'w') as f:\n",
    "\n",
    "# print(tweepy.Cursor(twitter_api.search,q = \"google\", since = \"2014-02-14\", until = \"2014-02-15\", lang = \"en\").items())\n",
    "# for tweet in tweepy.Cursor(twitter_api.search,q = \"google\", since = \"2014-02-14\", until = \"2014-02-15\", lang = \"en\").items():\n",
    "    # print(tweet)\n",
    "# for tweet in tweepy.Cursor(twitter_api.search, q='apple' + \" -rt\", include_retweets=False, since='2018-06-26', until = '2018-06-27').items(10000):\n",
    "    # print(tweet)\n",
    "        # f.write(json.dumps(tweet._json)+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "for tweet in Cursor(twitter_api.search, q=\"FamiliesBelongTogether OR immigration OR ChildrenInCages OR KeepFamiliesTogether -rt\",until = '2020-01-03').items(10000):\n",
    "    print(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}